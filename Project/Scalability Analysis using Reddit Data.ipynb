{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# (1 cores, 2gb per machine) x 3 = 3 cores\n",
    "\n",
    "# New API\n",
    "spark_session = SparkSession\\\n",
    "        .builder\\\n",
    "        .master(\"spark://192.168.2.147:7077\") \\\n",
    "        .appName(\"spark_trail\")\\\n",
    "        .config(\"spark.dynamicAllocation.enabled\", False)\\\n",
    "        .config(\"spark.shuffle.service.enabled\", False)\\\n",
    "        .config(\"spark.cores.max\", \"4\")\\\n",
    "        .getOrCreate()\n",
    "#.config(\"spark.executor.cores\",2)\\        \n",
    "# Old API (RDD)\n",
    "spark_context = spark_session.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_context.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading sample reddit comment data from HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"score\":-1,\"subreddit_id\":\"t5_vf2\",\"author_flair_text\":null,\"retrieved_on\":1425872352,\"name\":\"t1_c04w38g\",\"author_flair_css_class\":null,\"score_hidden\":false,\"ups\":-1,\"archived\":true,\"link_id\":\"t3_6udsz\",\"author\":\"xniphobe\",\"controversiality\":0,\"id\":\"c04w38g\",\"edited\":false,\"body\":\"lol...ingenious.\",\"distinguished\":null,\"created_utc\":\"1217548802\",\"subreddit\":\"nsfw\",\"parent_id\":\"t1_c04w378\",\"gilded\":0,\"downs\":0}\n"
     ]
    }
   ],
   "source": [
    "sample_comments = spark_context.textFile(\"hdfs://192.168.2.147:9000/RC_2008-08-3\")\n",
    "print(sample_comments.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1787877"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_comments.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weak scalability test\n",
    "- Fixing the number of comments to be processed by each core to be constant - 595959\n",
    "- Increase the number of cores and data size simultaneously\n",
    "\n",
    "### Top 10 /Most popular/frequent words from the comments - 1 core\n",
    "Starting with one core and load the dataset into HDFS which has :\n",
    "- Data Size: 346.626502 MB\n",
    "- Total comments: 595959\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark_session = SparkSession\\\n",
    "        .builder\\\n",
    "        .master(\"spark://192.168.2.147:7077\") \\\n",
    "        .appName(\"spark_trail\")\\\n",
    "        .config(\"spark.dynamicAllocation.enabled\", False)\\\n",
    "        .config(\"spark.shuffle.service.enabled\", False)\\\n",
    "        .getOrCreate()\n",
    "# Old API (RDD)\n",
    "spark_context = spark_session.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24.1 ms, sys: 4.89 ms, total: 29 ms\n",
      "Wall time: 46.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import json\n",
    "def split_comments(line):\n",
    "    line_json = json.loads(line)\n",
    "    return line_json['body'].lower().split(' ')\n",
    "\n",
    "words_of_comments = sample_comments.map(lambda line: split_comments(line))\n",
    "\n",
    "def add(a,b):\n",
    "    return a+b\n",
    "\n",
    "most_freq_comments = words_of_comments\\\n",
    ".flatMap(lambda w: w)\\\n",
    ".map(lambda w: (w,1))\\\n",
    ".reduceByKey(add)\n",
    "\n",
    "most_freq_comments.takeOrdered(10, key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 /Most popular/frequent words from the comments - 2 cores\n",
    "Adding one more core and add more data into HDFS which has :\n",
    "- Data Size: 661.14 MB\n",
    "- Total comments: 1191918\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 36.5 ms, sys: 3.49 ms, total: 39.9 ms\n",
      "Wall time: 49.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import json\n",
    "def split_comments(line):\n",
    "    line_json = json.loads(line)\n",
    "    return line_json['body'].lower().split(' ')\n",
    "\n",
    "words_of_comments = sample_comments.map(lambda line: split_comments(line))\n",
    "\n",
    "def add(a,b):\n",
    "    return a+b\n",
    "\n",
    "most_freq_comments = words_of_comments\\\n",
    ".flatMap(lambda w: w)\\\n",
    ".map(lambda w: (w,1))\\\n",
    ".reduceByKey(add)\n",
    "\n",
    "most_freq_comments.takeOrdered(10, key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 /Most popular/frequent words from the comments - 3 cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27.5 ms, sys: 853 µs, total: 28.3 ms\n",
      "Wall time: 48.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import json\n",
    "def split_comments(line):\n",
    "    line_json = json.loads(line)\n",
    "    return line_json['body'].lower().split(' ')\n",
    "\n",
    "words_of_comments = sample_comments.map(lambda line: split_comments(line))\n",
    "\n",
    "def add(a,b):\n",
    "    return a+b\n",
    "\n",
    "most_freq_comments = words_of_comments\\\n",
    ".flatMap(lambda w: w)\\\n",
    ".map(lambda w: (w,1))\\\n",
    ".reduceByKey(add)\n",
    "\n",
    "most_freq_comments.takeOrdered(10, key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 /Most popular/frequent words from the comments - 4 cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28.9 ms, sys: 634 µs, total: 29.5 ms\n",
      "Wall time: 49 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import json\n",
    "def split_comments(line):\n",
    "    line_json = json.loads(line)\n",
    "    return line_json['body'].lower().split(' ')\n",
    "\n",
    "words_of_comments = sample_comments.map(lambda line: split_comments(line))\n",
    "\n",
    "def add(a,b):\n",
    "    return a+b\n",
    "\n",
    "most_freq_comments = words_of_comments\\\n",
    ".flatMap(lambda w: w)\\\n",
    ".map(lambda w: (w,1))\\\n",
    ".reduceByKey(add)\n",
    "\n",
    "most_freq_comments.takeOrdered(10, key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 /Most popular/frequent words from the comments - 1 core STRONG SCALABILITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 39.1 ms, sys: 14.3 ms, total: 53.4 ms\n",
      "Wall time: 2min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import json\n",
    "def split_comments(line):\n",
    "    line_json = json.loads(line)\n",
    "    return line_json['body'].lower().split(' ')\n",
    "\n",
    "words_of_comments = sample_comments.map(lambda line: split_comments(line))\n",
    "\n",
    "def add(a,b):\n",
    "    return a+b\n",
    "\n",
    "most_freq_comments = words_of_comments\\\n",
    ".flatMap(lambda w: w)\\\n",
    ".map(lambda w: (w,1))\\\n",
    ".reduceByKey(add)\n",
    "\n",
    "most_freq_comments.takeOrdered(10, key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 /Most popular/frequent words from the comments - 2 cores STRONG SCALABILITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.2 ms, sys: 12.5 ms, total: 33.7 ms\n",
      "Wall time: 1min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import json\n",
    "def split_comments(line):\n",
    "    line_json = json.loads(line)\n",
    "    return line_json['body'].lower().split(' ')\n",
    "\n",
    "words_of_comments = sample_comments.map(lambda line: split_comments(line))\n",
    "\n",
    "def add(a,b):\n",
    "    return a+b\n",
    "\n",
    "most_freq_comments = words_of_comments\\\n",
    ".flatMap(lambda w: w)\\\n",
    ".map(lambda w: (w,1))\\\n",
    ".reduceByKey(add)\n",
    "\n",
    "most_freq_comments.takeOrdered(10, key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 /Most popular/frequent words from the comments - 3 cores STRONG SCALABILITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.1 ms, sys: 9.36 ms, total: 31.5 ms\n",
      "Wall time: 48.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import json\n",
    "def split_comments(line):\n",
    "    line_json = json.loads(line)\n",
    "    return line_json['body'].lower().split(' ')\n",
    "\n",
    "words_of_comments = sample_comments.map(lambda line: split_comments(line))\n",
    "\n",
    "def add(a,b):\n",
    "    return a+b\n",
    "\n",
    "most_freq_comments = words_of_comments\\\n",
    ".flatMap(lambda w: w)\\\n",
    ".map(lambda w: (w,1))\\\n",
    ".reduceByKey(add)\n",
    "\n",
    "most_freq_comments.takeOrdered(10, key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 /Most popular/frequent words from the comments - 4 cores STRONG SCALABILITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.6 ms, sys: 4.35 ms, total: 26 ms\n",
      "Wall time: 33.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import json\n",
    "def split_comments(line):\n",
    "    line_json = json.loads(line)\n",
    "    return line_json['body'].lower().split(' ')\n",
    "\n",
    "words_of_comments = sample_comments.map(lambda line: split_comments(line))\n",
    "\n",
    "def add(a,b):\n",
    "    return a+b\n",
    "\n",
    "most_freq_comments = words_of_comments\\\n",
    ".flatMap(lambda w: w)\\\n",
    ".map(lambda w: (w,1))\\\n",
    ".reduceByKey(add)\n",
    "\n",
    "most_freq_comments.takeOrdered(10, key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To do, remove the empty characters, punctiations, escape characters: /n, /t, /r ... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of words that start with each letter of the alphabet - 1 core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 64.9 ms, sys: 16.3 ms, total: 81.2 ms\n",
      "Wall time: 3min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def processing_characters(word):\n",
    "    if len(word) > 0 and word is not None:\n",
    "        if word[0] is not None:\n",
    "            return (word[0],1)\n",
    "\n",
    "def processing_words(word):\n",
    "    if len(word) > 0 and word is not None:\n",
    "        return str(word)\n",
    "\n",
    "def processing_letters(word):\n",
    "    if str(word[0]).isalpha() == True:\n",
    "        return word\n",
    "\n",
    "words_of_comments = sample_comments.map(lambda line: split_comments(line))\n",
    "    \n",
    "character_distribution = words_of_comments\\\n",
    ".flatMap(lambda w: processing_words(w))\\\n",
    ".filter(lambda w: processing_letters(w))\\\n",
    ".map(lambda x: processing_characters(x))\\\n",
    ".reduceByKey(add)\n",
    "\n",
    "character_distribution.takeOrdered(10, key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of words that start with each letter of the alphabet - 3 cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def processing_characters(word):\n",
    "    if len(word) > 0 and word is not None:\n",
    "        if word[0] is not None:\n",
    "            return (word[0],1)\n",
    "\n",
    "def processing_words(word):\n",
    "    if len(word) > 0 and word is not None:\n",
    "        return str(word)\n",
    "\n",
    "def processing_letters(word):\n",
    "    if str(word[0]).isalpha() == True:\n",
    "        return word\n",
    "\n",
    "words_of_comments = sample_comments.map(lambda line: split_comments(line))\n",
    "    \n",
    "character_distribution = words_of_comments\\\n",
    ".flatMap(lambda w: processing_words(w))\\\n",
    ".filter(lambda w: processing_letters(w))\\\n",
    ".map(lambda x: processing_characters(x))\\\n",
    ".reduceByKey(add)\n",
    "\n",
    "character_distribution.takeOrdered(10, key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create columns for month, date and year, based on created_utc attribute using UDF.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "\n",
    "sqlContext = SQLContext(spark_context)\n",
    "df = sqlContext.read.json(\"hdfs://192.168.2.147:9000/sample_data.json\").cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    " from pyspark.sql.functions import col, from_unixtime\n",
    "\n",
    "\n",
    "df = df.withColumn(\n",
    "    'datetime',\n",
    "   from_unixtime(df.created_utc / 1000 ** 3,\"yyyy-MM-dd HH:mm:ss:SSS\")\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- author: string (nullable = true)\n",
      " |-- author_cakeday: boolean (nullable = true)\n",
      " |-- author_flair_css_class: string (nullable = true)\n",
      " |-- author_flair_text: string (nullable = true)\n",
      " |-- body: string (nullable = true)\n",
      " |-- can_gild: boolean (nullable = true)\n",
      " |-- controversiality: long (nullable = true)\n",
      " |-- created_utc: long (nullable = true)\n",
      " |-- distinguished: string (nullable = true)\n",
      " |-- edited: string (nullable = true)\n",
      " |-- gilded: long (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- is_submitter: boolean (nullable = true)\n",
      " |-- link_id: string (nullable = true)\n",
      " |-- parent_id: string (nullable = true)\n",
      " |-- permalink: string (nullable = true)\n",
      " |-- retrieved_on: long (nullable = true)\n",
      " |-- score: long (nullable = true)\n",
      " |-- stickied: boolean (nullable = true)\n",
      " |-- subreddit: string (nullable = true)\n",
      " |-- subreddit_id: string (nullable = true)\n",
      " |-- datetime: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|created_utc|\n",
      "+-----------+\n",
      "| 1506816257|\n",
      "| 1506816019|\n",
      "| 1506816105|\n",
      "| 1506816129|\n",
      "| 1506816160|\n",
      "| 1506816183|\n",
      "| 1506816244|\n",
      "| 1506816274|\n",
      "| 1506816299|\n",
      "| 1506816049|\n",
      "| 1506816057|\n",
      "| 1506816221|\n",
      "| 1506816235|\n",
      "| 1506816017|\n",
      "| 1506816012|\n",
      "| 1506816089|\n",
      "| 1506816167|\n",
      "| 1506816301|\n",
      "| 1506816077|\n",
      "| 1506816108|\n",
      "+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('created_utc').distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top ten active users\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- author: string (nullable = true)\n",
      " |-- author_cakeday: boolean (nullable = true)\n",
      " |-- author_flair_css_class: string (nullable = true)\n",
      " |-- author_flair_text: string (nullable = true)\n",
      " |-- body: string (nullable = true)\n",
      " |-- can_gild: boolean (nullable = true)\n",
      " |-- controversiality: long (nullable = true)\n",
      " |-- created_utc: long (nullable = true)\n",
      " |-- distinguished: string (nullable = true)\n",
      " |-- edited: string (nullable = true)\n",
      " |-- gilded: long (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- is_submitter: boolean (nullable = true)\n",
      " |-- link_id: string (nullable = true)\n",
      " |-- parent_id: string (nullable = true)\n",
      " |-- permalink: string (nullable = true)\n",
      " |-- retrieved_on: long (nullable = true)\n",
      " |-- score: long (nullable = true)\n",
      " |-- stickied: boolean (nullable = true)\n",
      " |-- subreddit: string (nullable = true)\n",
      " |-- subreddit_id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 active users - 1 core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "sqlContext = SQLContext(spark_context)\n",
    "df = sqlContext.read.json(\"hdfs://192.168.2.147:9000/RC_2008-08-3\").cache()\n",
    "\n",
    "df[df['author']!='[deleted]'].groupBy('author').count().orderBy('count', ascending = False).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 active users - 2 cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|         author|count|\n",
      "+---------------+-----+\n",
      "|           Ra__| 2714|\n",
      "|NoMoreNicksLeft| 2688|\n",
      "|   uteunawaytay| 2492|\n",
      "|         matts2| 2302|\n",
      "|       duskglow| 2262|\n",
      "|     malcontent| 2058|\n",
      "|        fingers| 1934|\n",
      "|     mexicodoug| 1848|\n",
      "|     dirtymoney| 1746|\n",
      "|      glengyron| 1648|\n",
      "+---------------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "CPU times: user 21.1 ms, sys: 0 ns, total: 21.1 ms\n",
      "Wall time: 45.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "sqlContext = SQLContext(spark_context)\n",
    "df = sqlContext.read.json(\"hdfs://192.168.2.147:9000/RC_2008-08-2\").cache()\n",
    "\n",
    "df[df['author']!='[deleted]'].groupBy('author').count().orderBy('count', ascending = False).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 active users - 4 cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|         author|count|\n",
      "+---------------+-----+\n",
      "|           Ra__| 5428|\n",
      "|NoMoreNicksLeft| 5376|\n",
      "|   uteunawaytay| 4984|\n",
      "|         matts2| 4604|\n",
      "|       duskglow| 4524|\n",
      "|     malcontent| 4116|\n",
      "|        fingers| 3868|\n",
      "|     mexicodoug| 3696|\n",
      "|     dirtymoney| 3492|\n",
      "|      glengyron| 3296|\n",
      "+---------------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "CPU times: user 18.2 ms, sys: 4.33 ms, total: 22.5 ms\n",
      "Wall time: 42 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "sqlContext = SQLContext(spark_context)\n",
    "df = sqlContext.read.json(\"hdfs://192.168.2.147:9000/RC_2008-08-4\").cache()\n",
    "\n",
    "df[df['author']!='[deleted]'].groupBy('author').count().orderBy('count', ascending = False).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 active users - 1 cores STRONG SCALABILITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|         author|count|\n",
      "+---------------+-----+\n",
      "|           Ra__| 4071|\n",
      "|NoMoreNicksLeft| 4032|\n",
      "|   uteunawaytay| 3738|\n",
      "|         matts2| 3453|\n",
      "|       duskglow| 3393|\n",
      "|     malcontent| 3087|\n",
      "|        fingers| 2901|\n",
      "|     mexicodoug| 2772|\n",
      "|     dirtymoney| 2619|\n",
      "|      glengyron| 2472|\n",
      "+---------------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "CPU times: user 21.8 ms, sys: 12.1 ms, total: 33.9 ms\n",
      "Wall time: 1min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "sqlContext = SQLContext(spark_context)\n",
    "df = sqlContext.read.json(\"hdfs://192.168.2.147:9000/RC_2008-08-3\").cache()\n",
    "\n",
    "df[df['author']!='[deleted]'].groupBy('author').count().orderBy('count', ascending = False).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 active users - 2 cores STRONG SCALABILITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|         author|count|\n",
      "+---------------+-----+\n",
      "|           Ra__| 4071|\n",
      "|NoMoreNicksLeft| 4032|\n",
      "|   uteunawaytay| 3738|\n",
      "|         matts2| 3453|\n",
      "|       duskglow| 3393|\n",
      "|     malcontent| 3087|\n",
      "|        fingers| 2901|\n",
      "|     mexicodoug| 2772|\n",
      "|     dirtymoney| 2619|\n",
      "|      glengyron| 2472|\n",
      "+---------------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "CPU times: user 22.3 ms, sys: 18.2 ms, total: 40.5 ms\n",
      "Wall time: 50.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "sqlContext = SQLContext(spark_context)\n",
    "df = sqlContext.read.json(\"hdfs://192.168.2.147:9000/RC_2008-08-3\").cache()\n",
    "\n",
    "df[df['author']!='[deleted]'].groupBy('author').count().orderBy('count', ascending = False).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 active users - 3 cores STRONG SCALABILITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|         author|count|\n",
      "+---------------+-----+\n",
      "|           Ra__| 4071|\n",
      "|NoMoreNicksLeft| 4032|\n",
      "|   uteunawaytay| 3738|\n",
      "|         matts2| 3453|\n",
      "|       duskglow| 3393|\n",
      "|     malcontent| 3087|\n",
      "|        fingers| 2901|\n",
      "|     mexicodoug| 2772|\n",
      "|     dirtymoney| 2619|\n",
      "|      glengyron| 2472|\n",
      "+---------------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "CPU times: user 20.8 ms, sys: 9 µs, total: 20.8 ms\n",
      "Wall time: 41.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "sqlContext = SQLContext(spark_context)\n",
    "df = sqlContext.read.json(\"hdfs://192.168.2.147:9000/RC_2008-08-3\").cache()\n",
    "\n",
    "df[df['author']!='[deleted]'].groupBy('author').count().orderBy('count', ascending = False).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 active users - 4 cores STRONG SCALABILITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|         author|count|\n",
      "+---------------+-----+\n",
      "|           Ra__| 4071|\n",
      "|NoMoreNicksLeft| 4032|\n",
      "|   uteunawaytay| 3738|\n",
      "|         matts2| 3453|\n",
      "|       duskglow| 3393|\n",
      "|     malcontent| 3087|\n",
      "|        fingers| 2901|\n",
      "|     mexicodoug| 2772|\n",
      "|     dirtymoney| 2619|\n",
      "|      glengyron| 2472|\n",
      "+---------------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "CPU times: user 14.5 ms, sys: 4.71 ms, total: 19.2 ms\n",
      "Wall time: 34.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "sqlContext = SQLContext(spark_context)\n",
    "df = sqlContext.read.json(\"hdfs://192.168.2.147:9000/RC_2008-08-3\").cache()\n",
    "\n",
    "df[df['author']!='[deleted]'].groupBy('author').count().orderBy('count', ascending = False).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}